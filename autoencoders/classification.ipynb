{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bee79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778541b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/home/karmpatel/miniconda3/envs/torch310/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "# from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# from model import VAE\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# from models.vae import VAE\n",
    "from models.vae2 import VAE\n",
    "\n",
    "\n",
    "from models.auto_encoder import AutoEncoder\n",
    "from dataloader.animal_faces import AnimalfaceDataset\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664bb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e153a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/14630 [00:00<?, ?it/s]/data3/home/karmpatel/miniconda3/envs/torch310/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14630/14630 [18:36<00:00, 13.10it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((width,width))])\n",
    "train_data = AnimalfaceDataset(transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "def5d345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [02:38<00:00,  9.48it/s]\n"
     ]
    }
   ],
   "source": [
    "val_data = AnimalfaceDataset(transform=transform, type=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4799390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2048, 3, 128, 128]), torch.Size([2048]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 2048\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "x, y = next(iter(train_loader))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c53fc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "width = 128\n",
    "feature_size = 128\n",
    "filters = [3, 16, 32, 64, 128, 256]\n",
    "kernel_sizes = [4, 4, 4, 4, 4]\n",
    "strides = [2,2,2,2,2]\n",
    "paddings = [0,0,0,0,0]\n",
    "output_paddings = [0,1,0,0,0]\n",
    "# paddings = [0,0]\n",
    "return_only_liner = 0\n",
    "dropout_prob = 0.3\n",
    "\n",
    "if return_only_liner:\n",
    "    conv_ip_size = (3, 128, 128)\n",
    "    hidden_sizes = [128*128*3, 4096, feature_size]\n",
    "else:\n",
    "    conv_ip_size = (256,2,2)\n",
    "#     hidden_sizes = [conv_ip_size[0]*conv_ip_size[1]*conv_ip_size[2], 256*3, 256, feature_size]\n",
    "    hidden_sizes = [conv_ip_size[0]*conv_ip_size[1]*conv_ip_size[2], feature_size]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc5242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 1024]\n",
      "torch.Size([2048, 3, 128, 128]) torch.Size([2048, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 63, 63]             784\n",
      "              ReLU-2           [-1, 16, 63, 63]               0\n",
      "            Conv2d-3           [-1, 32, 30, 30]           8,224\n",
      "              ReLU-4           [-1, 32, 30, 30]               0\n",
      "            Conv2d-5           [-1, 64, 14, 14]          32,832\n",
      "              ReLU-6           [-1, 64, 14, 14]               0\n",
      "            Conv2d-7            [-1, 128, 6, 6]         131,200\n",
      "              ReLU-8            [-1, 128, 6, 6]               0\n",
      "            Conv2d-9            [-1, 256, 2, 2]         524,544\n",
      "             ReLU-10            [-1, 256, 2, 2]               0\n",
      "          Dropout-11                 [-1, 1024]               0\n",
      "           Linear-12                  [-1, 128]         131,200\n",
      "          Encoder-13                  [-1, 128]               0\n",
      "          Dropout-14                  [-1, 128]               0\n",
      "           Linear-15                 [-1, 1024]         132,096\n",
      "        Unflatten-16            [-1, 256, 2, 2]               0\n",
      "  ConvTranspose2d-17            [-1, 128, 6, 6]         524,416\n",
      "             ReLU-18            [-1, 128, 6, 6]               0\n",
      "  ConvTranspose2d-19           [-1, 64, 14, 14]         131,136\n",
      "             ReLU-20           [-1, 64, 14, 14]               0\n",
      "  ConvTranspose2d-21           [-1, 32, 30, 30]          32,800\n",
      "             ReLU-22           [-1, 32, 30, 30]               0\n",
      "  ConvTranspose2d-23           [-1, 16, 63, 63]           8,208\n",
      "             ReLU-24           [-1, 16, 63, 63]               0\n",
      "  ConvTranspose2d-25          [-1, 3, 128, 128]             771\n",
      "          Decoder-26          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 1,658,211\n",
      "Trainable params: 1,658,211\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 4.13\n",
      "Params size (MB): 6.33\n",
      "Estimated Total Size (MB): 10.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ae = AutoEncoder(feature_size=feature_size, conv_ip_size=conv_ip_size, filters=filters, \n",
    "                 kernel_sizes=kernel_sizes,strides=strides,output_paddings=output_paddings, \n",
    "                 paddings=paddings, hiddens_sizes=hidden_sizes, return_only_liner=return_only_liner, droput_prob=dropout_prob).to(device)\n",
    "op, enc = ae(x.to(device))\n",
    "print(op.shape, enc.shape)\n",
    "\n",
    "# summary(ae, (3,32,32), device=\"cuda\")\n",
    "summary(ae, (3,width,width), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e4dfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.load_state_dict(torch.load( f\"ckpts/autoencoder_{feature_size}_1l.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b555dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 1024]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [38, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m vae \u001b[38;5;241m=\u001b[39m VAE(feature_size\u001b[38;5;241m=\u001b[39mfeature_size, conv_ip_size\u001b[38;5;241m=\u001b[39mconv_ip_size, filters\u001b[38;5;241m=\u001b[39mfilters, \n\u001b[1;32m      2\u001b[0m                  kernel_sizes\u001b[38;5;241m=\u001b[39mkernel_sizes,strides\u001b[38;5;241m=\u001b[39mstrides,output_paddings\u001b[38;5;241m=\u001b[39moutput_paddings, \n\u001b[1;32m      3\u001b[0m                  paddings\u001b[38;5;241m=\u001b[39mpaddings, hiddens_sizes\u001b[38;5;241m=\u001b[39mhidden_sizes, return_only_liner\u001b[38;5;241m=\u001b[39mreturn_only_liner, \n\u001b[1;32m      4\u001b[0m           droput_prob\u001b[38;5;241m=\u001b[39mdropout_prob, n_samples\u001b[38;5;241m=\u001b[39mn_samples)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m op, enc, mu, logvar \u001b[38;5;241m=\u001b[39m vae(x\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(op\u001b[38;5;241m.\u001b[39mshape, enc\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/VAE/autoencoders/models/vae2.py:144\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m--> 144\u001b[0m     enc, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m    145\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(enc)\n\u001b[1;32m    146\u001b[0m     x_hat_avg \u001b[38;5;241m=\u001b[39m x_hat\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/VAE/autoencoders/models/vae2.py:58\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m     z \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer(x)\n\u001b[1;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mliner_layer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch310/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [38, 128]"
     ]
    }
   ],
   "source": [
    "vae = VAE(feature_size=feature_size, conv_ip_size=conv_ip_size, filters=filters, \n",
    "                 kernel_sizes=kernel_sizes,strides=strides,output_paddings=output_paddings, \n",
    "                 paddings=paddings, hiddens_sizes=hidden_sizes, return_only_liner=return_only_liner, \n",
    "          droput_prob=dropout_prob, n_samples=n_samples).to(device)\n",
    "op, enc, mu, logvar = vae(x.to(device))\n",
    "print(op.shape, enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e224162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72d333c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latents(ae, dataloader, device=torch.device(\"cuda\")):\n",
    "    temp, labels = [], []\n",
    "    for X,y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            ae.eval()\n",
    "            enc = ae.encoder(X)\n",
    "            temp.append(enc)\n",
    "            labels.extend(y)\n",
    "    latents = torch.cat(temp).to(device)\n",
    "    return latents, torch.tensor(labels)\n",
    "\n",
    "def get_latents_vae(vae, dataloader, device=torch.device(\"cuda\")):\n",
    "    temp, labels = [], []\n",
    "    for X,y in tqdm(dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            vae.eval()\n",
    "            enc, mu, logvar = vae.encoder(X)\n",
    "            z = torch.concat((mu, logvar))\n",
    "            temp.append(z)\n",
    "            labels.extend(y)\n",
    "    latents = torch.cat(temp).to(device)\n",
    "    return latents, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c0c7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  5.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14630, 128]) torch.Size([1500, 128])\n"
     ]
    }
   ],
   "source": [
    "train_latents, train_labels = get_latents(ae, train_loader)\n",
    "val_latents, val_labels = get_latents(ae, val_loader)\n",
    "print(train_latents.shape, val_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latents, train_labels = get_latents_vae(vae, train_loader)\n",
    "val_latents, val_labels = get_latents_vae(vae, val_loader)\n",
    "print(train_latents.shape, val_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd8868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latents = train_latents[:100]\n",
    "# x_hat = ae.decoder(latents.to(device))\n",
    "# generated_grid = make_grid(x_hat, nrow=10)\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(generated_grid.permute(1,2,0).detach().to(cpu_device).numpy())\n",
    "# # save_image(generated_grid,f\"images/vae_generated_{feature_size}_1l_{beta}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7345bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, hiddens_sizes = [2048, 1024, 512, 256, 3]):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         hidden_layers = []\n",
    "#         for i in range(len(hiddens_sizes)-1):\n",
    "#             hidden_layers.append(nn.Linear(hiddens_sizes[i], hiddens_sizes[i+1]))\n",
    "# #             hidden_layers.append(nn.Dropout(p=0.15))\n",
    "#             if i < len(hiddens_sizes)-2:\n",
    "#                 hidden_layers.append(nn.ReLU(True))\n",
    "            \n",
    "#         self.liner_layer = nn.Sequential(*(hidden_layers[:-1]))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         logits = self.liner_layer(x)\n",
    "#         y_log_probs = F.log_softmax(logits, dim=-1)\n",
    "#         y_pred = torch.argmax(y_log_probs, dim=-1)\n",
    "#         return logits, y_log_probs, y_pred\n",
    "    \n",
    "# cl = Classifier(hiddens_sizes = [feature_size,64,3]).to(device)\n",
    "# optim = torch.optim.Adam(cl.parameters(), lr=0.001)\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e144d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, ip_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        hidden_layers = []\n",
    "        self.fc1 = nn.Linear(ip_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ad088aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 1000\n",
    "# tqdm_obj = tqdm(range(n_epochs))\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# for epoch in tqdm_obj:\n",
    "#     # train\n",
    "#     cl.train()\n",
    "#     optim.zero_grad()\n",
    "#     logits, log_prob, y_pred = cl(train_latents)\n",
    "#     loss = loss_fn(logits, train_labels.type(torch.LongTensor).to(device))\n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "#     train_losses.append(loss)\n",
    "    \n",
    "#     # val\n",
    "#     cl.eval()\n",
    "#     with torch.no_grad():\n",
    "#         val_logits, val_log_prob, val_y_pred = cl(val_latents)\n",
    "#         val_loss = loss_fn(val_log_prob, val_labels.type(torch.LongTensor).to(device))\n",
    "#         val_losses.append(val_loss)\n",
    "    \n",
    "#     tqdm_obj.set_description_str(f\"Epoch: {epoch} Train loss: {loss} Val loss: {val_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "984bc67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create a TensorDataset from x and y\n",
    "dataset = TensorDataset(train_latents, train_labels)\n",
    "\n",
    "# Create a DataLoader from the dataset\n",
    "batch_size = 256  # Adjust the batch size according to your needs\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22575ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c74bb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Train loss: 0.10106989741325378 Val loss: 0.31500309705734253: 100%|██████████████████████████████████████████| 15/15 [00:01<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "tqdm_obj = tqdm(range(n_epochs))\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "cl = Classifier(ip_dim=feature_size).to(device)\n",
    "optim = torch.optim.Adam(cl.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in tqdm_obj:\n",
    "    for x,y in dataloader:\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        # train\n",
    "        cl.train()\n",
    "        optim.zero_grad()\n",
    "        logits = cl(x)\n",
    "        loss = loss_fn(logits, y.type(torch.LongTensor).to(device))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_losses.append(loss)\n",
    "    \n",
    "    # val\n",
    "    cl.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = cl(val_latents)\n",
    "        val_loss = loss_fn(val_logits, val_labels.type(torch.LongTensor).to(device))\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    tqdm_obj.set_description_str(f\"Epoch: {epoch} Train loss: {loss} Val loss: {val_loss}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb106138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa259712150>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_cpu = lambda arr: [each.detach().to(cpu_device) for each in arr]\n",
    "# plt.plot(to_cpu(train_losses))\n",
    "plt.plot(to_cpu(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d2298a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8947)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y_pred = torch.argmax(val_logits, dim=-1)\n",
    "torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)(val_y_pred.to(cpu_device), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f8743ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[447,  32,  21],\n",
       "        [ 16, 458,  26],\n",
       "        [ 33,  30, 437]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchmetrics.ConfusionMatrix(task=\"multiclass\", num_classes=3)(val_y_pred.to(cpu_device), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef47ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch310]",
   "language": "python",
   "name": "conda-env-torch310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
