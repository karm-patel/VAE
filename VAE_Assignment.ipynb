{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e9d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eafbf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/home/karmpatel/miniconda3/envs/torch310/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "# from natsort import natsorted\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# from model import VAE\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22281d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "device = torch.device(\"cuda\")\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa67f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = \"autoencoders/dataset\"\n",
    "DATA_PATH = os.path.join(DATA_DIR_PATH, 'animal-faces')\n",
    "DATA_URL = 'https://www.kaggle.com/datasets/andrewmvd/animal-faces/download?datasetVersionNumber=1'\n",
    "device = torch.device(\"cuda\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "class AnimalfaceDataset(Dataset):\n",
    "    def __init__(self, transform, type='train', label_dict = {\"dog\":0, \"cat\":1, \"wild\":2} , img_width=128) -> None:\n",
    "        self.transform = transform\n",
    "        # self.root_dir specifies weather you are at afhq/train or afhq/val directory\n",
    "        self.label_dict = label_dict\n",
    "        self.root_dir = os.path.join(DATA_PATH, type)\n",
    "        assert os.path.exists(self.root_dir), \"Check for the dataset, it is not where it should be. If not present, you can download it by clicking above DATA_URL\"\n",
    "        subdir = os.listdir(self.root_dir)\n",
    "        self.image_names = []\n",
    "        \n",
    "        for category in subdir:\n",
    "            subdir_path = os.path.join(self.root_dir, category)\n",
    "            self.image_names+=os.listdir(subdir_path)\n",
    "\n",
    "        self.img_arr = torch.zeros((len(self.image_names), 3, img_width ,img_width))\n",
    "        self.labels = torch.zeros(len(self.image_names))\n",
    "            \n",
    "        for i,img_name in enumerate(tqdm(self.image_names)):\n",
    "            label = self.label_dict[img_name.split(\"_\")[1]]\n",
    "            img_path = os.path.join(self.root_dir, img_name.split(\"_\")[1], img_name)\n",
    "            # Load image and convert it to RGB\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            # Apply transformations to the image\n",
    "            img = self.transform(img)\n",
    "            self.img_arr[i] = img\n",
    "            self.labels[i] = label\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.img_arr[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3289f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 128\n",
    "train_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((width,width))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f6390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/14630 [00:00<?, ?it/s]/data3/home/karmpatel/miniconda3/envs/torch310/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      " 29%|███████████████████████████████▎                                                                              | 4172/14630 [08:17<13:20, 13.06it/s]"
     ]
    }
   ],
   "source": [
    "train_data = AnimalfaceDataset(transform=train_transform, img_width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((width,width))])\n",
    "val_data = AnimalfaceDataset(transform=val_transform, type=\"val\", img_width=width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e8c49",
   "metadata": {},
   "source": [
    "# Q1 - GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f71d05",
   "metadata": {},
   "source": [
    "# Q2 - Vanila - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5f4378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_vae(nn.Module):\n",
    "    def __init__(self, filters,  kernel_sizes,  strides, hiddens_sizes, paddings, \n",
    "                 return_only_conv=False, return_only_liner=False, droput_prob=0.1, curr_device=\"cuda\", n_samples=5):\n",
    "        super(Encoder_vae, self).__init__()\n",
    "        \n",
    "        self.curr_device = curr_device\n",
    "        self.return_only_conv = return_only_conv\n",
    "        self.return_only_liner = return_only_liner\n",
    "        self.n_samples = n_samples\n",
    "        conv_layers = []\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            conv_layers.append(nn.Conv2d(filters[i], filters[i+1], kernel_sizes[i], strides[i], paddings[i]))\n",
    "            conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        self.conv_layer = nn.Sequential(*conv_layers)\n",
    "\n",
    "        hidden_layers = []\n",
    "        hiddens_sizes_cpy = hiddens_sizes.copy()\n",
    "        hiddens_sizes_cpy[-1] = hiddens_sizes_cpy[-1]*2 # mu and sigma\n",
    "        for i in range(len(hiddens_sizes_cpy)-1):\n",
    "            hidden_layers.append(nn.Dropout(p=droput_prob))\n",
    "            hidden_layers.append(nn.Linear(hiddens_sizes_cpy[i], hiddens_sizes_cpy[i+1]))\n",
    "            if i < len(hiddens_sizes_cpy)-2:\n",
    "                hidden_layers.append(nn.ReLU(True))\n",
    "        self.liner_layer = nn.Sequential(*hidden_layers)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        if self.return_only_conv:\n",
    "            x = self.conv_layer(x)\n",
    "            x = x.flatten(start_dim=1)\n",
    "            z = x\n",
    "        elif self.return_only_liner:\n",
    "            x = x.flatten(start_dim=1)\n",
    "            x = self.liner_layer(x)\n",
    "            z = x\n",
    "        else:\n",
    "            x = self.conv_layer(x)\n",
    "            x = x.flatten(start_dim=1)\n",
    "            x = self.liner_layer(x)\n",
    "            z = x\n",
    "\n",
    "        z_dim = z.shape[-1]//2\n",
    "        mu, log_var = z[:, :z_dim], z[:, z_dim:]\n",
    "        sigma = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn((z_dim, self.n_samples)).to(torch.device(self.curr_device))\n",
    "        z = []\n",
    "        for i in range(self.n_samples):\n",
    "            z.append(mu + sigma*eps[:,i])\n",
    "        z = torch.stack(z)\n",
    "        return z, mu, log_var\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, conv_op_size,  filters, kernel_sizes, strides, output_paddings, \n",
    "                   paddings, hiddens_sizes, return_only_conv=False, return_only_liner=False, droput_prob=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.return_only_conv = return_only_conv\n",
    "        self.return_only_liner = return_only_liner\n",
    "        print(hiddens_sizes)\n",
    "        \n",
    "        hidden_layers = []\n",
    "        for i in range(len(hiddens_sizes)-1):\n",
    "            hidden_layers.append(nn.Dropout(p=droput_prob))\n",
    "            hidden_layers.append(nn.Linear(hiddens_sizes[i], hiddens_sizes[i+1]))\n",
    "            if i < len(hiddens_sizes)-2:\n",
    "                hidden_layers.append(nn.ReLU(True))\n",
    "        \n",
    "        self.liner_layer = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            conv_layers.append(nn.ConvTranspose2d(filters[i], filters[i+1], kernel_sizes[i], \n",
    "                                                  stride=strides[i], output_padding=output_paddings[i], padding=paddings[i]))\n",
    "            if i < len(kernel_sizes)-1:\n",
    "                conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        self.conv_layer = nn.Sequential(*conv_layers)        \n",
    "        self.unflatten = nn.Unflatten(dim=2, unflattened_size=conv_op_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.return_only_conv:\n",
    "            x = self.unflatten(x)\n",
    "            x = self.conv_layer(x)\n",
    "            x = torch.sigmoid(x)\n",
    "            return x\n",
    "        elif self.return_only_liner:\n",
    "            # print(x.shape)\n",
    "            x = self.liner_layer(x)\n",
    "            x = self.unflatten(x)\n",
    "            x = torch.sigmoid(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.liner_layer(x)\n",
    "            x = self.unflatten(x)\n",
    "            ans = []\n",
    "            for each in x:\n",
    "                ans.append(self.conv_layer(each))\n",
    "            x_hat = torch.stack(ans)\n",
    "            x_hat = torch.sigmoid(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, feature_size=2048, conv_ip_size=(32, 14, 14), filters = [3,12,24,48,128],  \n",
    "                 kernel_sizes = [3, 3, 3, 3], strides = [2, 2, 2, 2], output_paddings = [0,0,0,0], \n",
    "                 paddings = [0,0,0,0], hiddens_sizes = [2048, 1024, 512, 256, 3], return_only_conv=False, \n",
    "                 return_only_liner=False, droput_prob=0.2, n_samples=5):\n",
    "        '''\n",
    "        if return_only_liner=True, then conv_ip_size = (3, 128, 128) and hidden_sizes [3*128*128, ... , features_size]\n",
    "        '''\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder_vae = Encoder_vae(filters=filters, \n",
    "                               kernel_sizes=kernel_sizes,strides=strides, hiddens_sizes=hiddens_sizes, \n",
    "                               return_only_conv=return_only_conv, return_only_liner=return_only_liner, \n",
    "                               droput_prob=droput_prob, paddings=paddings, n_samples=5)\n",
    "        \n",
    "        self.decoder = Decoder(conv_op_size=conv_ip_size, filters=filters[::-1], \n",
    "                               kernel_sizes=kernel_sizes[::-1],strides=strides[::-1], output_paddings=output_paddings[::-1],\n",
    "                                 paddings=paddings[::-1], hiddens_sizes=hiddens_sizes[::-1] , return_only_liner=return_only_liner)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        enc, mu, logvar = self.encoder_vae(x)\n",
    "        x_hat = self.decoder(enc)\n",
    "        x_hat_avg = x_hat.mean(dim=0)\n",
    "        return x_hat_avg, enc, mu, logvar\n",
    "\n",
    "    def loss_fn(self, x, x_hat, mu, logvar, beta=1):\n",
    "        mse_loss = nn.MSELoss(reduction=\"sum\")(x, x_hat)\n",
    "        kl_loss = torch.sum(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "        return mse_loss+ beta*kl_loss, mse_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ed76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8252a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bec593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5a81cf",
   "metadata": {},
   "source": [
    "# Q4 - Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30fbd665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filters,  kernel_sizes,  strides, hiddens_sizes, paddings, \n",
    "                 return_only_conv=False, return_only_liner=False, droput_prob=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.return_only_conv = return_only_conv\n",
    "        self.return_only_liner = return_only_liner\n",
    "        conv_layers = []\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            conv_layers.append(nn.Conv2d(filters[i], filters[i+1], kernel_sizes[i], strides[i], paddings[i]))\n",
    "            conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "        self.conv_layer = nn.Sequential(*conv_layers)\n",
    "\n",
    "        hidden_layers = []\n",
    "        for i in range(len(hiddens_sizes)-1):\n",
    "            hidden_layers.append(nn.Dropout(p=droput_prob))\n",
    "            hidden_layers.append(nn.Linear(hiddens_sizes[i], hiddens_sizes[i+1]))\n",
    "            if i < len(hiddens_sizes)-2:\n",
    "                hidden_layers.append(nn.ReLU(True))\n",
    "\n",
    "            # hidden_layers.append(nn.ReLU(True))\n",
    "        self.liner_layer = nn.Sequential(*hidden_layers)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.return_only_conv:\n",
    "            x = self.conv_layer(x)\n",
    "            x = x.flatten(start_dim=1)\n",
    "            return x\n",
    "        elif self.return_only_liner:\n",
    "            x = x.flatten(start_dim=1)\n",
    "            x = self.liner_layer(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.conv_layer(x)\n",
    "            x = x.flatten(start_dim=1)\n",
    "            x = self.liner_layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, conv_op_size,  filters, kernel_sizes, strides, output_paddings, \n",
    "                   paddings, hiddens_sizes, return_only_conv=False, return_only_liner=False, droput_prob=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.return_only_conv = return_only_conv\n",
    "        self.return_only_liner = return_only_liner\n",
    "        print(hiddens_sizes)\n",
    "        \n",
    "        hidden_layers = []\n",
    "        for i in range(len(hiddens_sizes)-1):\n",
    "            hidden_layers.append(nn.Dropout(p=droput_prob))\n",
    "            hidden_layers.append(nn.Linear(hiddens_sizes[i], hiddens_sizes[i+1]))\n",
    "            if i < len(hiddens_sizes)-2:\n",
    "                hidden_layers.append(nn.ReLU(True))\n",
    "            # hidden_layers.append(nn.ReLU(True))\n",
    "        \n",
    "        self.liner_layer = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        conv_layers = []\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            conv_layers.append(nn.ConvTranspose2d(filters[i], filters[i+1], kernel_sizes[i], \n",
    "                                                  stride=strides[i], output_padding=output_paddings[i], padding=paddings[i]))\n",
    "            if i < len(kernel_sizes)-1:\n",
    "                conv_layers.append(nn.ReLU(True))\n",
    "\n",
    "\n",
    "        self.conv_layer = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=conv_op_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.return_only_conv:\n",
    "            x = self.unflatten(x)\n",
    "            x = self.conv_layer(x)\n",
    "            x = torch.sigmoid(x)\n",
    "            return x\n",
    "        elif self.return_only_liner:\n",
    "            # print(x.shape)\n",
    "            x = self.liner_layer(x)\n",
    "            x = self.unflatten(x)\n",
    "            x = torch.sigmoid(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.liner_layer(x)\n",
    "            x = self.unflatten(x)\n",
    "            x = self.conv_layer(x)\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, feature_size=2048, conv_ip_size=(32, 14, 14), filters = [3,12,24,48,128],  \n",
    "                 kernel_sizes = [3, 3, 3, 3], strides = [2, 2, 2, 2], output_paddings = [0,0,0,0], \n",
    "                 paddings = [0,0,0,0], hiddens_sizes = [2048, 1024, 512, 256, 3], return_only_conv=False, \n",
    "                 return_only_liner=False, droput_prob=0.2):\n",
    "        '''\n",
    "        if return_only_liner=True, then conv_ip_size = (3, 128, 128) and hidden_sizes [3*128*128, ... , features_size]\n",
    "        '''\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(filters=filters, \n",
    "                               kernel_sizes=kernel_sizes,strides=strides, hiddens_sizes=hiddens_sizes, \n",
    "                               return_only_conv=return_only_conv, return_only_liner=return_only_liner, \n",
    "                               droput_prob=droput_prob, paddings=paddings)\n",
    "        \n",
    "        self.decoder = Decoder(conv_op_size=conv_ip_size, filters=filters[::-1], \n",
    "                               kernel_sizes=kernel_sizes[::-1],strides=strides[::-1], output_paddings=output_paddings[::-1],\n",
    "                                 paddings=paddings[::-1], hiddens_sizes=hiddens_sizes[::-1] , return_only_liner=return_only_liner)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        enc = self.encoder(x)\n",
    "        x = self.decoder(enc)\n",
    "        return x, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c848cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch310]",
   "language": "python",
   "name": "conda-env-torch310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
